{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arkanayudhistira/bangkit/blob/main/Digit_Sign_Language_Detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "KLzgn5utHEoT"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import string\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator, array_to_img\n",
        "from keras.layers import Dense, Conv2D , MaxPool2D , Flatten , Dropout , BatchNormalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OgD9hdJZAuL8",
        "outputId": "2ec4f2f8-aed4-4932-b9d5-af9961bc4f66"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "CA7HNw3hBmWv"
      },
      "outputs": [],
      "source": [
        "TRAINING_DIR = '/content/drive/MyDrive/Capstone Project/Sign Language Digit/Training'\n",
        "VALIDATION_DIR = '/content/drive/MyDrive/Capstone Project/Sign Language Digit/Testing'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_val_generators(TRAINING_DIR, VALIDATION_DIR):\n",
        "  train_datagen = ImageDataGenerator(rescale=1./255,\n",
        "                                     rotation_range=10, \n",
        "                                     zoom_range=0.1, \n",
        "                                     width_shift_range=0.1,  \n",
        "                                     height_shift_range=0.1)\n",
        "\n",
        "  train_generator = train_datagen.flow_from_directory(directory=TRAINING_DIR,\n",
        "                                                      batch_size=50,\n",
        "                                                      class_mode='categorical',\n",
        "                                                      target_size=(150, 150))\n",
        "\n",
        "  validation_datagen = ImageDataGenerator(rescale=1/255)\n",
        "\n",
        "  validation_generator = validation_datagen.flow_from_directory(directory=VALIDATION_DIR,\n",
        "                                                                batch_size=50,\n",
        "                                                                class_mode='categorical',\n",
        "                                                                target_size=(150, 150))\n",
        "  \n",
        "  return train_generator, validation_generator"
      ],
      "metadata": {
        "id": "c0jbOuOIXr5J"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xTu37BOHEMna"
      },
      "outputs": [],
      "source": [
        "def train_val_generators(training_images, training_labels, validation_images, validation_labels):\n",
        "  training_images = np.expand_dims(training_images, axis=3)\n",
        "  validation_images = np.expand_dims(validation_images, axis=3)\n",
        "\n",
        "  train_datagen = ImageDataGenerator(rescale=1./255,\n",
        "                                     rotation_range=10, \n",
        "                                     zoom_range=0.1, \n",
        "                                     width_shift_range=0.1,  \n",
        "                                     height_shift_range=0.1) \n",
        "\n",
        "  train_generator = train_datagen.flow(x=training_images,\n",
        "                                       y=training_labels,\n",
        "                                       batch_size=32) \n",
        "\n",
        "  validation_datagen = ImageDataGenerator(rescale = 1./255)\n",
        "\n",
        "  validation_generator = validation_datagen.flow(x=validation_images,\n",
        "                                                 y=validation_labels,\n",
        "                                                 batch_size=32) \n",
        "\n",
        "  return train_generator, validation_generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "16gFkeeVFFy-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88286901-2d73-43cc-9e2d-772d700fcbb9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1644 images belonging to 10 classes.\n",
            "Found 418 images belonging to 10 classes.\n"
          ]
        }
      ],
      "source": [
        "train_generator, validation_generator = train_val_generators(TRAINING_DIR, VALIDATION_DIR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "JvbjbbGFFJZw"
      },
      "outputs": [],
      "source": [
        "def create_model():\n",
        "\n",
        "  model = tf.keras.models.Sequential([tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(150, 150, 3)),\n",
        "                                      tf.keras.layers.MaxPooling2D(2, 2),\n",
        "                                      tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "                                      tf.keras.layers.MaxPooling2D(2,2),\n",
        "                                      tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "                                      tf.keras.layers.MaxPooling2D(2,2),\n",
        "                                      tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "                                      tf.keras.layers.MaxPooling2D(2,2),\n",
        "                                      tf.keras.layers.Flatten(),                                 \n",
        "                                      tf.keras.layers.Dense(256, activation='relu'),\n",
        "                                      tf.keras.layers.Dropout(0.2),\n",
        "                                      tf.keras.layers.Dense(10, activation='softmax')])\n",
        "\n",
        "  model.compile(optimizer = 'adam',\n",
        "                loss = 'categorical_crossentropy',\n",
        "                metrics=['accuracy']) \n",
        "  \n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "doqj-P6UG3cp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0445fdb9-b530-4bc7-a778-7e17f2a80d8f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "33/33 [==============================] - 20s 414ms/step - loss: 2.3103 - accuracy: 0.1071 - val_loss: 2.2991 - val_accuracy: 0.1005\n",
            "Epoch 2/30\n",
            "33/33 [==============================] - 13s 405ms/step - loss: 2.2324 - accuracy: 0.1575 - val_loss: 1.7336 - val_accuracy: 0.4330\n",
            "Epoch 3/30\n",
            "33/33 [==============================] - 13s 399ms/step - loss: 1.5737 - accuracy: 0.4428 - val_loss: 0.7704 - val_accuracy: 0.7560\n",
            "Epoch 4/30\n",
            "33/33 [==============================] - 13s 396ms/step - loss: 1.0420 - accuracy: 0.6496 - val_loss: 0.4943 - val_accuracy: 0.8684\n",
            "Epoch 5/30\n",
            "33/33 [==============================] - 13s 399ms/step - loss: 0.7804 - accuracy: 0.7269 - val_loss: 0.3166 - val_accuracy: 0.9091\n",
            "Epoch 6/30\n",
            "33/33 [==============================] - 13s 401ms/step - loss: 0.6101 - accuracy: 0.8011 - val_loss: 0.2148 - val_accuracy: 0.9402\n",
            "Epoch 7/30\n",
            "33/33 [==============================] - 13s 395ms/step - loss: 0.5416 - accuracy: 0.8169 - val_loss: 0.2038 - val_accuracy: 0.9378\n",
            "Epoch 8/30\n",
            "33/33 [==============================] - 13s 400ms/step - loss: 0.4586 - accuracy: 0.8510 - val_loss: 0.1550 - val_accuracy: 0.9378\n",
            "Epoch 9/30\n",
            "33/33 [==============================] - 13s 395ms/step - loss: 0.3632 - accuracy: 0.8869 - val_loss: 0.1262 - val_accuracy: 0.9713\n",
            "Epoch 10/30\n",
            "33/33 [==============================] - 13s 404ms/step - loss: 0.3066 - accuracy: 0.8996 - val_loss: 0.1145 - val_accuracy: 0.9569\n",
            "Epoch 11/30\n",
            "33/33 [==============================] - 13s 399ms/step - loss: 0.2458 - accuracy: 0.9215 - val_loss: 0.1618 - val_accuracy: 0.9593\n",
            "Epoch 12/30\n",
            "33/33 [==============================] - 13s 395ms/step - loss: 0.2608 - accuracy: 0.9118 - val_loss: 0.1093 - val_accuracy: 0.9665\n",
            "Epoch 13/30\n",
            "33/33 [==============================] - 13s 396ms/step - loss: 0.2282 - accuracy: 0.9288 - val_loss: 0.1138 - val_accuracy: 0.9689\n",
            "Epoch 14/30\n",
            "33/33 [==============================] - 13s 394ms/step - loss: 0.1986 - accuracy: 0.9355 - val_loss: 0.1028 - val_accuracy: 0.9785\n",
            "Epoch 15/30\n",
            "33/33 [==============================] - 13s 399ms/step - loss: 0.1842 - accuracy: 0.9428 - val_loss: 0.0654 - val_accuracy: 0.9833\n",
            "Epoch 16/30\n",
            "33/33 [==============================] - 13s 400ms/step - loss: 0.1680 - accuracy: 0.9471 - val_loss: 0.0706 - val_accuracy: 0.9833\n",
            "Epoch 17/30\n",
            "33/33 [==============================] - 13s 398ms/step - loss: 0.1486 - accuracy: 0.9550 - val_loss: 0.0920 - val_accuracy: 0.9809\n",
            "Epoch 18/30\n",
            "33/33 [==============================] - 14s 412ms/step - loss: 0.1367 - accuracy: 0.9532 - val_loss: 0.0733 - val_accuracy: 0.9809\n",
            "Epoch 19/30\n",
            "33/33 [==============================] - 13s 404ms/step - loss: 0.1444 - accuracy: 0.9586 - val_loss: 0.0636 - val_accuracy: 0.9833\n",
            "Epoch 20/30\n",
            "33/33 [==============================] - 13s 400ms/step - loss: 0.1195 - accuracy: 0.9653 - val_loss: 0.0992 - val_accuracy: 0.9809\n",
            "Epoch 21/30\n",
            "33/33 [==============================] - 13s 393ms/step - loss: 0.1436 - accuracy: 0.9538 - val_loss: 0.0872 - val_accuracy: 0.9737\n",
            "Epoch 22/30\n",
            "33/33 [==============================] - 13s 398ms/step - loss: 0.1171 - accuracy: 0.9678 - val_loss: 0.0649 - val_accuracy: 0.9928\n",
            "Epoch 23/30\n",
            "33/33 [==============================] - 13s 396ms/step - loss: 0.0934 - accuracy: 0.9726 - val_loss: 0.0859 - val_accuracy: 0.9833\n",
            "Epoch 24/30\n",
            "33/33 [==============================] - 13s 393ms/step - loss: 0.0997 - accuracy: 0.9726 - val_loss: 0.0798 - val_accuracy: 0.9856\n",
            "Epoch 25/30\n",
            "33/33 [==============================] - 13s 403ms/step - loss: 0.1185 - accuracy: 0.9672 - val_loss: 0.0354 - val_accuracy: 0.9928\n",
            "Epoch 26/30\n",
            "33/33 [==============================] - 13s 394ms/step - loss: 0.0973 - accuracy: 0.9678 - val_loss: 0.0502 - val_accuracy: 0.9904\n",
            "Epoch 27/30\n",
            "33/33 [==============================] - 13s 397ms/step - loss: 0.0875 - accuracy: 0.9672 - val_loss: 0.0623 - val_accuracy: 0.9833\n",
            "Epoch 28/30\n",
            "33/33 [==============================] - 13s 399ms/step - loss: 0.0774 - accuracy: 0.9714 - val_loss: 0.0752 - val_accuracy: 0.9785\n",
            "Epoch 29/30\n",
            "33/33 [==============================] - 13s 398ms/step - loss: 0.0811 - accuracy: 0.9720 - val_loss: 0.0513 - val_accuracy: 0.9856\n",
            "Epoch 30/30\n",
            "33/33 [==============================] - 13s 394ms/step - loss: 0.0608 - accuracy: 0.9824 - val_loss: 0.0661 - val_accuracy: 0.9833\n"
          ]
        }
      ],
      "source": [
        "model = create_model()\n",
        "\n",
        "history = model.fit(train_generator,\n",
        "                    epochs=30,\n",
        "                    validation_data=validation_generator)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "2KL-kLdg3DG2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Le4VVPqhffyv"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Digit Sign Language Detection.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}